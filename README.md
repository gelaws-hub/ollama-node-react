# Ollama Streaming Chatbot

A simple **Node.js** application to run **Ollama** locally and stream the AI-generated responses in real time. The project consists of:

* **Vite** âš¡ for the frontend (React-based UI)
* **Express.js** ğŸš€ for the backend (handles API & socket connections)
* **Socket.io** ğŸ”— as the communication protocol (real-time streaming)

---

## ğŸš€ Features

* **Run Ollama locally** and generate AI responses
* **Stream responses** in real-time using WebSockets
* **Interactive chat UI** with Markdown rendering
* **Efficient, lightweight, and developer-friendly**

---

## ğŸ› ï¸ Setup & Installation

### 1ï¸âƒ£ Prerequisites

* [Node.js](https://nodejs.org/) (v16+ recommended)
* [Ollama](https://ollama.ai/) installed and running
* [Vite](https://vitejs.dev/) for frontend development

### 2ï¸âƒ£ Clone the Repository

```bash
$ git clone https://github.com/gelaws-hub/ollama-node-react.git
$ cd ollama-node-react
```

### 3ï¸âƒ£ Install Dependencies

#### Backend

```bash
$ cd server
$ npm install
```

#### Frontend

```bash
$ cd client
$ npm install
```

### 4ï¸âƒ£ Run the Project

#### Start the Backend Server

```bash
$ cd server
$ npm run start
```

#### Start the Frontend (Vite Dev Server)

```bash
$ cd client
$ npm run dev
```

Now, visit **`http://localhost:5173`** to see your chatbot in action! ğŸ‰

---

## âš™ï¸ Configuration

### **Backend Environment Variables** (`.env`)

```env
ORIGIN_URL=http://localhost:5173,https://anotherfrontend.com
MODEL_NAME=deepseek-r1:7b
PORT=3000
```

### **Frontend Configuration**

Copy and paste as `.env` to your backend domain:

```js
VITE_SERVER_URL=https://localhost:3000
```

---

## ğŸ“œ License

This project is **open-source** and available under the  **MIT License** .

---

## ğŸ¤ Contributing

Got ideas to improve this project? PRs and contributions are welcome! ğŸ”¥

ğŸ“Œ  **Author** : [Ibnu Fadhil](https://github.com/ge)
